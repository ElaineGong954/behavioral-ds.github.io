<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Testing | Behavioral Data Science</title><link>https://www.behavioral-ds.ml/blogpost/</link><atom:link href="https://www.behavioral-ds.ml/blogpost/index.xml" rel="self" type="application/rss+xml"/><description>Testing</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2021</copyright><lastBuildDate>Fri, 02 Jul 2021 00:00:00 +0000</lastBuildDate><image><url>https://www.behavioral-ds.ml/img/logo.png</url><title>Testing</title><link>https://www.behavioral-ds.ml/blogpost/</link></image><item><title>We spent six years scouring billions of links, and found the web is both expanding and shrinking</title><link>https://www.behavioral-ds.ml/blogpost/online_diversity/</link><pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/online_diversity/</guid><description>&lt;p>The online world is continuously expanding — always aggregating more services, more users and more activity. Last year, the number of websites registered on the &amp;ldquo;.com&amp;rdquo; domain &lt;a href="https://www.verisign.com/en_US/channel-resources/domain-registry-products/zone-file/index.xhtml">surpassed 150,000,000&lt;/a>.&lt;/p>
&lt;p>However, more than a quarter of a century since its first commercial use, the growth of the online world is now slowing down in some key categories.&lt;/p>
&lt;p>We conducted a multi-year research project analysing global trends in online diversity and dominance. &lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249993">Our research&lt;/a>, published in Public Library of Science, is the first to reveal some long-term trends in how businesses compete in the age of the web.&lt;/p>
&lt;p>We saw a dramatic consolidation of attention towards a shrinking (but increasingly dominant) group of online organisations. So, while there is still growth in the functions, features and applications offered on the web, the number of entities providing these functions is shrinking.&lt;/p>
&lt;h2 id="web-diversity-nosedives">Web diversity nosedives&lt;/h2>
&lt;p>We analysed more than six billion user comments from the social media website Reddit dating back to 2006, as well as 11.8 billion Twitter posts from as far back as 2011. In total, our research used a massive 5.6Tb trove of data from more than a decade of global activity.&lt;/p>
&lt;p>This dataset was more than four times the size of the original data from the Hubble Space Telescope, which helped Brian Schmidt and colleagues do their &lt;a href="https://theconversation.com/nobel-prize-win-tells-us-the-universe-is-accelerating-what-does-that-mean-3753">Nobel-prize winning work&lt;/a> in 1998 to prove &lt;a href="https://iopscience.iop.org/article/10.1086/300499/meta">the universe's expansion is accelerating&lt;/a>.&lt;/p>
&lt;p>With the Reddit posts, we analysed all the links to other sites and online services — more than one billion in total — to understand the dynamics of link growth, dominance and diversity through the decade.&lt;/p>
&lt;p>We used a measure of link &amp;ldquo;uniqueness&amp;rdquo;. On this scale, 1 represents maximum diversity (all links have their own domain) and 0 is minimum diversity (all links are on one domain, such as &amp;ldquo;youtube.com&amp;rdquo;).&lt;/p>
&lt;p>A decade ago, there was a much greater variety of domains within links posted by users of Reddit, with more than 20 different domains for every 100 random links users posted. Now there are only about five different domains for every 100 links posted.&lt;/p>
&lt;p>&lt;img src="fig1.png" alt="Our Reddit analysis showed the pool of top-performing sources online is shrinking.">&lt;/p>
&lt;p>In fact, between 60-70% of all attention on key social media platforms is focused towards just ten popular domains.&lt;/p>
&lt;p>Beyond social media platforms, we also studied linkage patterns across the web, looking at almost 20 billion links over three years. These results reinforced the &amp;ldquo;rich are getting richer&amp;rdquo; online.&lt;/p>
&lt;p>The authority, influence and visibility of the top 1,000 global websites (as measured by network centrality or PageRank) is growing every month, at the expense of all other sites.&lt;/p>
&lt;h2 id="app-diversity-is-on-the-rise">App diversity is on the rise&lt;/h2>
&lt;p>The web started as a source of innovation, new ideas and inspiration — a technology that opened up the playing field. It's now also becoming a medium that actually stifles competition and promotes monopolies and the dominance of a few players.&lt;/p>
&lt;p>Our findings resolve a long-running paradox about the nature of the web: does it help grow businesses, jobs and investment? Or does it make it harder to get ahead by letting anyone and everyone join the game? The answer, it turns out, is it does both.&lt;/p>
&lt;p>While the diversity of sources is in decline, there is a countervailing force of continually increasing functionality with new services, products and applications — such as music streaming services (Spotify), file sharing programs (Dropbox) and messaging platforms (Messenger, Whatsapp and Snapchat).&lt;/p>
&lt;p>&lt;img src="fig2.png" alt="Functional diversity grows continuously online.">&lt;/p>
&lt;h2 id="website-infant-mortality">Website ‘infant mortality&amp;rsquo;&lt;/h2>
&lt;p>Another major finding was the dramatic increase in the &amp;ldquo;infant mortality&amp;rdquo; rate of websites — with the big kids on the block guarding their turf more staunchly than ever.&lt;/p>
&lt;p>We examined new domains that were continually referenced or linked-to in social media after their first appearance. We found that while almost 40% of the domains created 2006 were active five years on, only a little more than 3% of those created in 2015 remain active today.&lt;/p>
&lt;p>The dynamics of online competition are becoming clearer and clearer. And the loss of diversity is concerning. Unlike the natural world, there are no sanctuaries; competition is part of both nature and business.&lt;/p>
&lt;p>Our study has profound implications for business leaders, investors and governments everywhere. It shows the network effects of the web don't just apply to online businesses. They have permeated the entire economy and are rewriting many previously accepted rules of economics.&lt;/p>
&lt;p>For example, the idea that businesses can maintain a competitive advantage based on where they are physically located is increasingly tenuous. Meanwhile, there's new opportunities for companies to set up shop from anywhere in the world and serve a global customer base that's both mainstream and niche.&lt;/p>
&lt;p>&lt;img src="fig3.png" alt="Functional diversity grows continuously online.">&lt;/p>
&lt;p>The best way to encourage diversity is to have more global online businesses focused on providing diverse services, by addressing consumers&amp;rsquo; increasingly niche needs.&lt;/p>
&lt;p>In Australia, we're starting to see this through homegrown companies such as &lt;a href="https://www.canva.com/">Canva&lt;/a>, &lt;a href="https://safetyculture.com/">SafetyCulture&lt;/a> and &lt;a href="https://iwonder.com/">iWonder&lt;/a>. Hopefully many more will appear in the decade ahead.&lt;/p>
&lt;p>&lt;em>This article was first published by the authors on &lt;a href="https://theconversation.com/we-spent-six-years-scouring-billions-of-links-and-found-the-web-is-both-expanding-and-shrinking-159215">The Conversation&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Job Transitions in a Time of Automation and Labour Market Crises</title><link>https://www.behavioral-ds.ml/blogpost/job_transitions/</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/job_transitions/</guid><description>&lt;p>&lt;strong>Summary:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>We build a machine learning-based &lt;em>Job Transitions Recommender System&lt;/em> that can accurately predict the probability of transitioning between occupations. We showcase the system for workers forced to transition between jobs.&lt;/li>
&lt;li>The system is based on a novel data-driven method to measure the similarity between occupations based on their underlying skill profiles and real-time job ads.&lt;/li>
&lt;li>We also build a leading indicator of Artificial Intelligence adoption in Australian industries, outlining gaps, opportunities, and trends.&lt;/li>
&lt;li>For full technical details, please read the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>People are forced to change jobs as new technologies automate labour, production is moved abroad, and economic crises unfold. Successfully transitioning between jobs, however, requires leveraging current skills and acquiring others, which can falter if the skills gap is too large.&lt;/p>
&lt;p>In our latest research, &lt;a href="authors/nik-dawson/">Nik Dawson&lt;/a> and &lt;a href="authors/ma-rizoiu/">Marian-Andrei Rizoiu&lt;/a> (together with collaborator &lt;a href="https://research.unsw.edu.au/people/professor-mary-anne-williams">Mary-Anne Williams&lt;/a>) and I developed a novel method to measure the &amp;lsquo;distance&amp;rsquo; between &lt;em>sets of skills&lt;/em> using real-time job ads data. We then use these measures to build a recommender system that accurately predicts the probability of transitioning from one occupation to every other possible occupation. Intuitively, two occupations have a high probability of successfully transitioning when their skill sets are highly similar (i.e. the distance is small). For example, an Accountant has a high probability of transitioning to become a Financial Analyst because their skill sets are similar; whereas a Speech Therapist has a low transition probability to becoming a Financial Analyst as their skill sets are very different. This isn't to say that it's not possible. Rather, the skills gap is large, so the probability of successfully transitioning is diminished.&lt;/p>
&lt;h2 id="the-skill-space-method">The SKILL SPACE Method&lt;/h2>
&lt;p>&lt;strong>Distance between skills&lt;/strong>&lt;/p>
&lt;p>In order to measure the distance between occupations from their underlying skill sets, we first measure the distance between individual skills in job ads for each calendar year from 2012-2020. To achieve this bottom-up approach, we first use a method from Trade Economics, called ‘Revealed Comparative Advantage&amp;rsquo; (RCA), to identify how important an individual skill is to a job ad (i.e. comparative advantage). Then, after some normalisation, we calculate the pairwise similarity between every skill for each year. The image below shows the skill distance embeddings for the top 500 skills by posting frequency in 2018.&lt;/p>
&lt;p>&lt;img src="img/fig1A.png" alt="Figure 1A.">&lt;/p>
&lt;p>Here, each marker represents an individual skill that is coloured according to one of 13 clusters of highly similar skills. As seen in the Software Development cluster (see inset), highly similar skills cluster closely together, such as ‘Python&amp;rsquo; and ‘C++&amp;rsquo;. The skills map also provides useful insights, highlighting that specialised skills (such as ‘Software Development&amp;rsquo; and ‘Healthcare&amp;rsquo;) tend to lay toward the edges of the embedding, whereas more general and transferable skills lay toward the middle, acting as a ‘bridge&amp;rsquo; to specialist skills.&lt;/p>
&lt;p>&lt;strong>Distance between occupations&lt;/strong>&lt;/p>
&lt;p>Next, we use the pairwise skill distances to measure the distance between &lt;em>sets of skills&lt;/em>. In this example, we define sets of skills by their occupational groupings. But they can just as easily be defined by other groupings, such as companies, industries, or personalised skill sets. We calculate the distance between skill sets as the weighted average similarity between the individual skills in each set, where the weights correspond to the skill importance in their respective sets. The figure below visualises the distance between Australian occupations in 2018.&lt;/p>
&lt;p>&lt;img src="img/fig1B.png" alt="Figure 1B.">&lt;/p>
&lt;p>Each occupation is represented by a marker and coloured on a scale according to their automation susceptibility, as calculated by &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0040162516302244">Frey and Osborne&lt;/a> - dark blue represents low-risk probability and dark red shows high risk probability over the coming two decades. As seen in the magnified inset, similar occupations lie close together on the map. Further, occupations in low risk of automation tend to be characterised by non-routine, interpersonal, and/or high cognitive labour tasks; whereas occupations in high risk of automation tend to require routine, manual, and/or low cognitive labour tasks. For example, in the inset of the Figure above, a ‘Sheetmetal Trades Worker&amp;rsquo; is deemed to be at high risk of labour automation (82% probability) due to high levels of routine and manual labour tasks required by the occupation. However, the skill set demands of a ‘Sheetmetal Trades Worker&amp;rsquo; are highly similar to an ‘Industrial Designer&amp;rsquo;, which is considered at low risk of labour automation over the coming two decades (4% probability). Therefore, an ‘Industrial Designer&amp;rsquo; represents a transition opportunity for a ‘Sheetmetal Trades Worker&amp;rsquo; that leverages existing skills and protects against potential risks of technological labour automation.&lt;/p>
&lt;h2 id="constructing-a-job-transitions-recommender-system">Constructing a Job Transitions Recommender System&lt;/h2>
&lt;p>The SKILL SPACE Method described above achieves high levels of accuracy in predicting job transitions. However, these are symmetric measures and we know that &lt;strong>job transitions are &lt;em>asymmetric&lt;/em>&lt;/strong> - it is more difficult to transition between jobs in one direction than the other. Therefore, transitions are determined by more than the symmetric distance between skill sets; other factors, such as educational requirements and experience demands, contribute to these asymmetries.&lt;/p>
&lt;p>We account for the asymmetries between job transitions by training a machine learning classifier model that combines the SKILL SPACE distance measures with other labour market variables from job ads data and employment statistics (see the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a> for full details). Our machine learning model is trained against a dataset of ‘actual&amp;rsquo; (i.e. real life) job transitions from an Australian longitudinal household survey. We then apply the trained model to predict the probability for every possible occupational transition in the dataset - described as the transition probability between a ‘source&amp;rsquo; and a ‘target&amp;rsquo; occupation. This creates the &lt;em>Transitions Map&lt;/em>, for which a subset of 20 occupations can be seen in the figure below.&lt;/p>
&lt;p>&lt;img src="img/fig2.png" alt="Figure 2.">&lt;/p>
&lt;p>The coloured heatmap shows the transition probabilities (‘source&amp;rsquo; occupations are in columns and ‘targets&amp;rsquo; are in rows). Dark blue represents higher transition probabilities and lighter blue shows lower probabilities, where the asymmetries between occupation pairs are clearly observed. For example, a ‘Finance Manager&amp;rsquo; has a higher probability of transitioning to become an ‘Accounting Clerk&amp;rsquo; than the reverse direction. Moreover, transitioning to some occupations is generally easier (for example, ‘Bar Attendants and Baristas&amp;rsquo;) than others (‘Life Scientists&amp;rsquo;). The dendrogram (the lines on the left and top of the chart) illustrates the hierarchical clusters of occupations where there is a clear divide between service-oriented professions and manual labour occupations.&lt;/p>
&lt;p>Our model achieves high levels of performance, &lt;strong>accurately predicting 76% of occupational transitions&lt;/strong>.&lt;/p>
&lt;h2 id="recommending-jobs-and-skills">Recommending Jobs and Skills&lt;/h2>
&lt;p>The &lt;em>Transitions Map&lt;/em> provides the basis for making qualified job transition recommendations. We call this the &lt;em>Job Transitions Recommender System&lt;/em>. In the figure below, we highlight the example of `Domestic Cleaners&amp;rsquo;, an occupation that has experienced significant declines in labour demand and employment levels during COVID-19 in Australia.&lt;/p>
&lt;p>&lt;img src="img/fig4.png" alt="Figure 4.">&lt;/p>
&lt;p>First, we use the &lt;em>Transitions Map&lt;/em> to recommend the occupations with the highest transition probabilities; these are the named occupations on the right side of the flow diagram, ordered in descending order of transition probability. Segment widths show the labour demand for each of the recommended occupations during the beginning of the COVID-19 period (measured by posting frequency). The segment colours represent the percentage change of posting frequency during March and April 2019 compared to the same months in 2020; dark red indicates a big decrease in job ad posts and dark blue indicates a big increase. The first six transition recommendations for ‘Domestic Cleaners&amp;rsquo; all experienced negative demand, which is unsurprising given that ‘non-essential&amp;rsquo; services were restricted in Australia during this period. However, the seventh recommendation, ‘Aged and Disabled Carers&amp;rsquo;, had significantly grown in demand during the beginning of the COVID-19 period and there was a high number of jobs advertised. Given that it is generally favorable to transition to high demand jobs, we selected ‘Aged and Disabled Carers&amp;rsquo; as the target occupation for this example.&lt;/p>
&lt;p>&lt;strong>Skill recommendations for target occupations&lt;/strong>&lt;/p>
&lt;p>We then take the &lt;em>Job Transitions Recommender System&lt;/em> a step further by incorporating skill recommendations. Transitioning to a new occupation generally requires developing new skills under time and resource constraints. Therefore, workers must prioritise which skills to develop. We argue that a worker should invest in developing a skill when (1) the &lt;strong>skill is important to the target occupation&lt;/strong> &lt;em>and&lt;/em> (2) the &lt;strong>distance to acquire the skill is large&lt;/strong> (that is, it is relatively difficult to acquire). In the case of the ‘Domestic Cleaner&amp;rsquo; in the figure above, the top recommended skills to assist in the transition to the ‘Aged and Disabled Carer&amp;rsquo; occupation are specialised patient care skills, such as ‘Patient Hygiene Assistance&amp;rsquo;. Conversely, the reasons &lt;em>not&lt;/em> to develop a skill are when (1) the &lt;strong>skill is not important&lt;/strong> &lt;em>or&lt;/em> (2) the &lt;strong>distance is small to the target occupation&lt;/strong>. The figure shows that while some ‘Aged and Disabled Carer&amp;rsquo; jobs require ‘Business Analysis&amp;rsquo; and ‘Finance&amp;rsquo; skills, these skills are of low importance for the ‘Aged and Disabled Carer&amp;rsquo; occupation, so they should not be prioritised. Similarly, skills such as ‘Ironing&amp;rsquo; and ‘Laundry&amp;rsquo; are required by ‘Aged and Disabled Carer&amp;rsquo; jobs but the distance is small, so it is likely that either a ‘Domestic Cleaner&amp;rsquo; already possesses these skills or they can easily acquire them.&lt;/p>
&lt;h2 id="developing-a-leading-indicator-of-ai-adoption">Developing a Leading Indicator of AI Adoption&lt;/h2>
&lt;p>The SKILL SPACE method can also be flexibly applied to other adjacent problems, such as identifying the extent of specific skill gaps of firms within industries and potential adoption of new technologies. Here, we develop a leading indicator for emerging technology adoption and potential labour market disruptions based on skills data, using Artificial Intelligence (AI) as an example. We select AI because of its potential impacts on transforming labour tasks and accelerating job transitions. By applying SKILL SPACE, we were able to measure the yearly similarities between an adaptive set of AI skills against each of the 19 Australian industries from 2013-2019 (see the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a> for technical details). As industry skill sets become more similar to AI skills, the skills gap is diminished and firms within industries are more likely to possess the skills to make productive use of AI technologies. The growth of AI skill similarity within industries is shown by the coloured areas of the radar chart below.&lt;/p>
&lt;p>&lt;img src="img/fig5.png" alt="Figure 5.">&lt;/p>
&lt;p>All industries have increased their similarity levels to AI skills. This highlights the growing importance of AI skills across the Australian labour market. However, the rates of similarity are unequally distributed. Some industries, such as ‘Finance and Insurance Services&amp;rsquo; and ‘Information Media and Telecommunications&amp;rsquo; command much higher rates of AI skill similarity. This indicates that not only are firms within these industries increasingly demanding AI skills but also that the AI skills gaps within these industries are much smaller. Therefore, it is likely that firms within these industries are adopting AI and making productive use of these technologies.&lt;/p>
&lt;p>Also noteworthy are the differences in growth rates toward AI skill similarity. As the figure above clearly shows, AI skill similarity has rapidly grown for some industries and more modestly for others. For instance, ‘Retail Trade&amp;rsquo; has experienced the highest levels of growth in similarity to AI skills, increasing by 407% from 2013 to 2019. The majority of this growth has occurred recently, which coincides with the launch of Amazon Australia in 2017. Since then, Amazon has swiftly hired thousands in Australia. This indicator can assist policy-makers and businesses to robustly monitor the growth of AI skills (or other emerging technologies), which acts as a proxy for AI adoption within industries.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>While the future of work remains unclear, change is inevitable. New technologies, economic crises, and other factors will continue to shift labour demands causing workers to move between jobs. If labour transitions occur efficiently, significant productivity and equity benefits arise at all levels of the labour market; if transitions are slow, or fail, significant costs are borne to both the State and the individual. Therefore, it's in the interests of workers, firms, and governments that labour transitions are efficient and effective. The methods and systems we put forward here could significantly improve the achievement of these goals.&lt;/p>
&lt;p>For the full technical details, please read the &lt;a href="https://arxiv.org/abs/2011.11801">pre-print&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Acknowledgements&lt;/strong>&lt;/p>
&lt;p>We thank &lt;a href="https://www.linkedin.com/in/bleditaska?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3Bvr7rCfoNTVGsSR9hhmd%2FhQ%3D%3D">Bledi Taska&lt;/a> and &lt;a href="https://ca.linkedin.com/in/dmiskulin">Davor Miskulin&lt;/a> from Burning Glass Technologies for generously providing the job advertisements data for this research and for their valuable feedback. We also thank &lt;a href="https://fr.linkedin.com/in/broecke-stijn-36ba2048">Stijn Broecke&lt;/a> and other colleagues from the OECD for their ongoing input and guidance in the development of this work.&lt;/p></description></item><item><title>User Analysis on reshare cascades about COVID-19</title><link>https://www.behavioral-ds.ml/blogpost/user_analysis/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://www.behavioral-ds.ml/blogpost/user_analysis/</guid><description>&lt;p>We demonstrate in this blog post a tutorial on applying the tools for analyzing online information diffusions about Twitter users, &lt;a href="https://github.com/behavioral-ds/BirdSpotter">&lt;code>birdspotter&lt;/code>&lt;/a> and &lt;a href="https://github.com/behavioral-ds/evently">&lt;code>evently&lt;/code>&lt;/a>.&lt;/p>
&lt;h2 id="dataset">Dataset&lt;/h2>
&lt;p>In this tutorial, we apply two tools for analyzing Twitter users, on a COVID-19 retweet dataset. The dataset
is curated by Chen, et al. One can obtain a copy of the tweet IDs from
their &lt;a href="https://github.com/echen102/COVID-19-TweetIDs">project&lt;/a>. We
only use the 31st of Janury sample of the whole dataset for
demonstration purpose. The tweets can be recovered by &lt;code>hydration&lt;/code>
from their IDs. We note that some tweets might have been deleted and in
the end we manage to get 69.2% (1,489,877) of the original tweets.&lt;/p>
&lt;h2 id="tools">Tools&lt;/h2>
&lt;p>While &lt;code>BirdSpotter&lt;/code> captures the social influence and botness of Twitter
users, &lt;code>evently&lt;/code> specifically models the temporal dynamics of online
information diffusion. We leverage information provided by the tools to
study the users in the COVID19 dataset.&lt;/p>
&lt;pre>&lt;code class="language-r">library(evently)
library(reticulate)
birdspotter &amp;lt;- import('birdspotter')
&lt;/code>&lt;/pre>
&lt;h2 id="preprocessing-tweets">Preprocessing tweets&lt;/h2>
&lt;p>At this step, we seek to extract diffusion cascades from the &lt;code>COVID-19&lt;/code>
dataset for analyzing user influence and botness. A diffusion cascade
consist of an initial tweet posted by a Twitter user and followed then
by a sereis of retweets. A function provided by &lt;code>evently&lt;/code> allows one to
obtain cascades from JSON formatted raw tweets. On the other hand, we
initialize a &lt;code>BirdSpotter&lt;/code> instance and compute the influence and
botness scores for all users in the
dataset.&lt;/p>
&lt;pre>&lt;code class="language-r">cascades &amp;lt;- parse_raw_tweets_to_cascades('corona_2020_01_31.jsonl', keep_user = T, keep_absolute_time = T)
bs &amp;lt;- birdspotter$BirdSpotter('corona_2020_01_31.jsonl')
labeled_users &amp;lt;- bs$getLabeledUsers()[, c('user_id', 'botness', 'influence')]
&lt;/code>&lt;/pre>
&lt;p>As we cannot publish &lt;code>corona_2020_01_31.jsonl&lt;/code> due to Twitter TOC, we
have stored the results and load them below&lt;/p>
&lt;pre>&lt;code class="language-r">load('corona_2020_01_31.rda')
labeled_users &amp;lt;- read.csv('corona_31_botness_influence.csv', stringsAsFactors = F,
colClasses=c(&amp;quot;character&amp;quot;,rep(&amp;quot;numeric&amp;quot;,3)))
&lt;/code>&lt;/pre>
&lt;p>We note that all user IDs have been encrypted. After obtaining the
results, let’s first conduct some simple measurements on users and
cascades.&lt;/p>
&lt;pre>&lt;code class="language-r">library(ggplot2)
# check the density of these two values
mean_bot &amp;lt;- mean(labeled_users$botness, na.rm = T)
ggplot(labeled_users, aes(botness)) +
stat_density(geom = 'line') +
geom_vline(xintercept = mean_bot, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_bot, y = 2, label= sprintf('mean: %s', round(mean_bot, 2))), color= 'red', angle=90, vjust=-0.11)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_inf &amp;lt;- mean(labeled_users$influence)
ggplot(labeled_users) +
stat_ecdf(aes(influence, 1 - ..y..)) +
scale_x_log10() +
scale_y_log10() +
ylab('CCDF') +
geom_vline(xintercept = mean_inf, linetype=2, color = 'red') +geom_text(data=data.frame(), aes(x = mean_inf, y = 1e-3, label= sprintf('mean: %s', round(mean_inf, 2))), color= 'red', angle=90, vjust=-0.11)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-2.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value &amp;lt;- mean(sapply(cascades, nrow))
ggplot(data.frame(size = sapply(cascades, nrow))) +
stat_ecdf(aes(size, 1 - ..y..)) +
scale_x_log10() + scale_y_log10() +
geom_vline(xintercept = mean_value, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) +
xlab('cascade size') +
ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-3.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value2 &amp;lt;- mean(sapply(cascades, function(c) c$time[nrow(c)]))
ggplot(data.frame(time = sapply(cascades, function(c) c$time[nrow(c)]))) +
stat_ecdf(aes(time, 1 - ..y..)) +
scale_x_continuous(trans = 'log1p', breaks = c(0, 100, 10000, 1000000), labels = c('0', '1e2', '1e4', '1e6')) +
scale_y_log10() +
geom_vline(xintercept = mean_value2, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value2, y = 1e-3, label= sprintf('mean: %s', round(mean_value2, 2))), color= 'red', angle=90, vjust=-0.11) +
xlab('cascade final event time')+
ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-4.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">mean_value &amp;lt;- mean(labeled_users$activity)
ggplot(data.frame(size = labeled_users$activity)) +
stat_ecdf(aes(size, 1 - ..y..)) +
scale_x_log10() +
scale_y_log10() +
geom_vline(xintercept = mean_value, linetype=2, color = 'red') +
geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('user activity')+ ylab('CCDF')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## Warning: Transformation introduced infinite values in continuous y-axis
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-4-5.png" alt="">&lt;!-- -->&lt;/p>
&lt;h2 id="retrain-the-bot-detector">Retrain the bot detector&lt;/h2>
&lt;p>If one find the botness scores are not accurate, &lt;code>birdspotter&lt;/code> provides
a relabeling tool and a retrain API to learn from the given relabeled
dataset&lt;/p>
&lt;pre>&lt;code class="language-r"># output a file for mannual labeling
bs$getBotAnnotationTemplate('users_to_label.csv')
# Once annotated the botness detector can be trained with
bs$trainClassifierModel('users_to_label.csv')
&lt;/code>&lt;/pre>
&lt;h2 id="fit-user-posted-cacsades-with-evently">Fit user posted cacsades with &lt;code>evently&lt;/code>&lt;/h2>
&lt;p>We model a group of cascades initiated by a particular user jointly and
treat the fitted model as a characterization of the user. In this
example, we select two users for comparison.&lt;/p>
&lt;pre>&lt;code class="language-r">selected_users &amp;lt;- c('369686755237813560', '174266868073402929')
# fit Hawkes process on cascades initiated by the selected users
user_cascades_fitted &amp;lt;- lapply(selected_users, function(user) {
# select cascades that are initiated by the &amp;quot;selected_user&amp;quot;
selected_cascades &amp;lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades)
# obtain the observation times;
# note 1580515200 is 1st Feb when the observation stopped
# as we only observed until the end of 31st Jan
times &amp;lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1])
# fit a model on the selected cascades;
fit_series(data = selected_cascades, model_type = 'mPL', observation_time = times, cores = 10)
})
user_cascades_SEISMIC_fitted &amp;lt;- lapply(selected_users, function(user) {
selected_cascades &amp;lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades)
times &amp;lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1])
fit_series(data = selected_cascades, model_type = 'SEISMIC',
observation_time = times)
})
# check the fitted kernel functions
plot_kernel_function(user_cascades_fitted) +
scale_color_discrete(labels = c(&amp;quot;@BobOngHugots&amp;quot;, &amp;quot;@Jaefans_Global&amp;quot;))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-6-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;p>The plot shows the fitted kernel functions of these two users which
reflect their time-decaying influence of attracting followers to reshare
their posts. We then demonstrate how to simulate new cascades&lt;/p>
&lt;pre>&lt;code class="language-r">set.seed(134841)
user_magnitude &amp;lt;- Filter(function(cascade) cascade$user[[1]] == selected_users[[1]], cascades)[[1]]$magnitude[1]
# simulate a new cascade from @BobOngHugots
sim_cascade &amp;lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude)
plot_event_series(cascade = sim_cascade, model = user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-7-1.png" alt="">&lt;!-- -->&lt;/p>
&lt;pre>&lt;code class="language-r">selected_cascade &amp;lt;- Filter(function(cascade) cascade$user[1] == selected_users[[1]], cascades)[[1]]
selected_time &amp;lt;- user_cascades_fitted[[1]]$observation_time[1]
# simulate a cascade with a &amp;quot;selected_cascade&amp;quot; from @BobOngHugots
sim_cascade &amp;lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude,
init_history = selected_cascade)
sprintf('%s new events simulated after cascade',
nrow(sim_cascade[[1]]) - nrow(selected_cascade))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] &amp;quot;25 new events simulated after cascade&amp;quot;
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">predict_final_popularity(user_cascades_fitted[[1]],
selected_cascade, selected_time)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 458.303
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r"># predict with SEISMIC model, assume we have fitted the SEISMIC model
predict_final_popularity(user_cascades_SEISMIC_fitted[[1]],
selected_cascade, selected_time)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 729.923
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">get_branching_factor(user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.7681281
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">get_viral_score(user_cascades_fitted[[1]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 7.407763
&lt;/code>&lt;/pre>
&lt;h2 id="visualize-users-in-a-latent-space">Visualize users in a latent space&lt;/h2>
&lt;p>We show a visualization of top 300 users posted most tweets using the
features returned by &lt;code>evently&lt;/code> along with the botness and influence
scores from &lt;code>birdspotter&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-r"># obtain observation times here again
times &amp;lt;- 1580515200 - sapply(cascades, function(cas) cas$absolute_time[1])
# indicate the grouping of each cascade with the user who started the cascade
names(cascades) &amp;lt;- sapply(cascades, function(cas) cas$user[1])
# fit Hawkes processes on all cascades first
fitted_corona &amp;lt;- group_fit_series(cascades, model_type = 'mPL', observation_time = times)
&lt;/code>&lt;/pre>
&lt;p>The fitting procedure takes quite long so we again load the pre-fitted
models here&lt;/p>
&lt;pre>&lt;code class="language-r">load('fitted_models.rda')
# choose the top 300 users who started most cacsades
selected_users &amp;lt;- labeled_users$user_id[labeled_users$user_id %in%
names(sort(sapply(fitted_corona, length), decreasing = T)[seq(300)])]
# gather the stats for these users
user_influences &amp;lt;- labeled_users$influence[labeled_users$user_id %in% selected_users]
user_botness &amp;lt;- labeled_users$botness[labeled_users$user_id %in% selected_users]
fitted_corona_selected &amp;lt;- fitted_corona[selected_users]
# get the features
features &amp;lt;- generate_features(fitted_corona_selected)
# compute distances between users using manhattan distance
features &amp;lt;- features[, -1] # remove the user id column
distances &amp;lt;- dist(features, method = 'manhattan')
library(tsne)
positions &amp;lt;- tsne(distances, k = 2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## sigma summary: Min. : 0.34223375605395 |1st Qu. : 0.457223801885988 |Median : 0.489891425900637 |Mean : 0.500483006369232 |3rd Qu. : 0.538593613780411 |Max. : 0.676779919259545 |
## Epoch: Iteration #100 error is: 14.1961110881254
## Epoch: Iteration #200 error is: 0.490122133064818
## Epoch: Iteration #300 error is: 0.474257867010761
## Epoch: Iteration #400 error is: 0.472067779170087
## Epoch: Iteration #500 error is: 0.471844181155159
## Epoch: Iteration #600 error is: 0.471798834134577
## Epoch: Iteration #700 error is: 0.471783207059971
## Epoch: Iteration #800 error is: 0.471632929621924
## Epoch: Iteration #900 error is: 0.47087861882558
## Epoch: Iteration #1000 error is: 0.470873765976829
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-r">df &amp;lt;- data.frame(x = positions[,1], y = positions[,2],
influence = user_influences, botness = user_botness)
df &amp;lt;- cbind(df, data.frame(botornot = ifelse(df$botness &amp;gt; 0.6, 'Bot', 'Not Bot')))
ggplot(df, aes(x, y, color = influence, shape = botornot, size = botornot)) +
geom_point() +
scale_shape_manual(values = c(15,1)) +
scale_size_manual(values = c(1.5, 1.2)) +
scale_color_gradient(low = '#56B1F7', high = '#132B43', trans = 'log10') +
theme_void() + labs(size = NULL, shape = NULL) +
theme(legend.direction = 'horizontal', legend.position = c(0.8, 0.2),
legend.key.size = unit(.3, 'cm'), legend.text = element_text(size = 6),
legend.title = element_text(size = 6), legend.spacing = unit(.05, 'cm'))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="README_files/figure-gfm/unnamed-chunk-10-1.png" alt="">&lt;!-- -->&lt;/p></description></item></channel></rss>